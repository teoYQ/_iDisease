{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#DATADIR = \"X:/Datasets/PetImages\"\n",
    "\n",
    "#CATEGORIES = [\"Dog\", \"Cat\"]\n",
    "dirname = ''\n",
    "train_path = os.path.join(dirname, 'OCT2017/train')\n",
    "train_normal_pth = os.path.join(train_path, 'NORMAL')\n",
    "train_dme_pth = os.path.join(train_path, 'DME')\n",
    "train_drusen_pth = os.path.join(train_path, 'DRUSEN')\n",
    "train_cnv_pth = os.path.join(train_path, 'CNV')\n",
    "    \n",
    "test_path = os.path.join(dirname, 'OCT2017/test')\n",
    "test_normal_pth = os.path.join(test_path, 'NORMAL')\n",
    "test_dme_pth = os.path.join(test_path, 'DME')\n",
    "test_drusen_pth = os.path.join(test_path, 'DRUSEN')\n",
    "test_cnv_pth = os.path.join(test_path, 'CNV')\n",
    "    \n",
    "val_path = os.path.join(dirname, 'OCT2017/val')\n",
    "val_normal_pth = os.path.join(val_path, 'NORMAL')\n",
    "val_dme_pth = os.path.join(val_path, 'DME')\n",
    "val_drusen_pth = os.path.join(val_path, 'DRUSEN')\n",
    "val_cnv_pth = os.path.join(val_path, 'CNV')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 26315/26315 [02:47<00:00, 157.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 11348/11348 [01:13<00:00, 154.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 8616/8616 [00:58<00:00, 147.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 37205/37205 [04:46<00:00, 129.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 300.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 260.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 282.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 242/242 [00:00<00:00, 262.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 275.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 249.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 258.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 249.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "testing_data= []\n",
    "val_data = []\n",
    "DATADIR = \"OCT2017/\"\n",
    "data = [\"train/\",\"test/\",\"val/\"]\n",
    "CATEGORIES = [\"NORMAL\",\"DME\",\"DRUSEN\",\"CNV\"]\n",
    "IMG_SIZE = 160\n",
    "def create_training_data():\n",
    "    for i in range(len(data)):\n",
    "    \n",
    "        for category in CATEGORIES:  # do dogs and cats\n",
    "        \n",
    "            path = os.path.join(DATADIR,data[i],category)  # create path to dogs and cats\n",
    "            class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "\n",
    "            for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
    "                try:\n",
    "                    if i == 0:\n",
    "                        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                        #print(img_array.shape)\n",
    "                        \n",
    "                        img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                        #print(img_array.shape)\n",
    "                        training_data.append([img_array, class_num])  # add this to our training_data\n",
    "                        \n",
    "                    if i == 1:\n",
    "                        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                        img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                        testing_data.append([img_array, class_num])  # add this to our training_data\n",
    "                    if i == 2:\n",
    "                        img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                        img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                        val_data.append([img_array, class_num])  # add this to our training_data\n",
    "                except Exception as e:  # in the interest in keeping the output clean...\n",
    "                    print(e)\n",
    "                    pass\n",
    "                \n",
    "                #except OSError as e:\n",
    "                #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "                #except Exception as e:\n",
    "                #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n",
    "X_TRAIN = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X_TRAIN.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "\n",
    "\n",
    "X_train =  np.asarray(X_TRAIN, dtype=np.float32)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-26a80b6f6a6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "print((X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0362b41f238e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 968/968 [00:00<00:00, 967839.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(val_data)\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(testing_data)\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for features,label in tqdm(testing_data):\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "#print(X_test[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        iaa.Invert(0.5),\n",
    "        iaa.Rotate((-45, 45)),\n",
    "        iaa.GaussianBlur(sigma=(0, 3.0)) \n",
    "        # blur images with a sigma of 0 to 3.0\n",
    "    ])\n",
    "def augment_seq(img):\n",
    "        seq_det = seq.to_deterministic()\n",
    "        aug_image = seq_det.augment_image(img)\n",
    "        return aug_image\n",
    "def augment(img):\n",
    "    pass\n",
    "    \n",
    "#images_aug = seq(images = X_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(images_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    " ############################################################################\n",
    "# TODO: Train a model on CIFAR-10.                 #\n",
    "############################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    model = model_init_fn()\n",
    "    opt = optimizer_init_fn()\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy']) \n",
    "    loss = model.fit(X_TRAIN, y_train, batch_size=64, epochs=num_epochs)\n",
    "    return model\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "############################################################################\n",
    "#                            END OF YOUR CODE                              #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "#val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83484, 160, 160, 3)\n",
      "(160, 160, 3)\n",
      "[[ 0. 17. 15. ... 22.  0.  0.]\n",
      " [ 1.  4.  9. ...  5.  1.  2.]\n",
      " [ 4. 18. 13. ... 17.  7.  4.]\n",
      " ...\n",
      " [ 2.  5.  7. ...  3. 12.  5.]\n",
      " [ 0.  2. 11. ... 12. 11.  6.]\n",
      " [ 2.  0.  8. ...  0.  9.  3.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "print(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n",
      "Train\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_part2() missing 1 required positional argument: 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5a4e36fda12f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m \u001b[0mtrained_params\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain_part2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_init_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_part2() missing 1 required positional argument: 'learning_rate'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Set up some global variables\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)\n",
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3,3), padding=\"same\")\n",
    "        self.relu1 = tf.keras.layers.Activation(\"relu\")\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, (3,3), padding=\"same\")\n",
    "        self.relu2 = tf.keras.layers.Activation(\"relu\")\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='valid')\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(32, (5,5), padding=\"same\")\n",
    "        self.relu3 = tf.keras.layers.Activation(\"relu\")\n",
    "        self.conv4 = tf.keras.layers.Conv2D(32, (5,5), padding=\"same\")\n",
    "        self.relu4 = tf.keras.layers.Activation(\"relu\")\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=None, padding='valid')\n",
    "        \n",
    "        \n",
    "        self.flatten2 = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(512)\n",
    "        self.relu5 = tf.keras.layers.Activation(\"relu\")\n",
    "        self.drop = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense2 = tf.keras.layers.Dense(10)\n",
    "        self.softmax = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.flatten2(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense2(x)\n",
    "    \n",
    "        x = self.softmax(x)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "# device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "print('Train')\n",
    "def train_part2(model_fn, init_fn, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    params = init_fn()  # Initialize the model parameters            \n",
    "    for e in range(epochs):    \n",
    "        for t, (x_np, y_np) in enumerate(train_dset):\n",
    "            # Run the graph on a batch of training data.\n",
    "            loss = training_step(model_fn, x_np, y_np, params, learning_rate)\n",
    "\n",
    "            # Periodically print the loss and check accuracy on the val set.\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, iteration %d, loss = %.4f' % (e, t, loss))\n",
    "                print('Validation:')\n",
    "                check_accuracy(val_dset, model_fn, params)\n",
    "    return params\n",
    "def training_step(model_fn, x, y, params, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        grad_params = tape.gradient(total_loss, params)\n",
    "\n",
    "        # Make a vanilla gradient descent step on all of the model parameters\n",
    "        # Manually update the weights using assign_sub()\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "                        \n",
    "        return total_loss\n",
    "    \n",
    "trained_params =train_part2(model_init_fn, epochs=num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
